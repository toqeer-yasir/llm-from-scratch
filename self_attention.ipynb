{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Fs9h3JMOqanN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your(x^1)\n",
        "[0.55, 0.87, 0.66], # journey(x^2)\n",
        "[0.57, 0.85, 0.64], # starts(x^3)\n",
        "[0.22, 0.58, 0.33], # with(x^4)\n",
        "[0.77, 0.25, 0.10], # one(x^5)\n",
        "[0.05, 0.80, 0.55]] # step(x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "AFhc-xSZsaTR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYHBKiIWWszn",
        "outputId": "08214e5b-03d7-4222-ec3d-3e0db9f2223e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in: int, d_out: int, context_length: int, dropout: int, num_heads: int, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0 # to strictly check number of output dimentins should be divisible by heads for their equal distribution in all the heads without floating pionts that is not acdeptable.\n",
        "        self.d_out = d_out\n",
        "        self.head_dim = d_out // num_heads # to divide output into all heads.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.context_length = context_length\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key= nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out) # to save the shape dimentins.\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # here x is our input torch tensor batch(embedding of a sentence tokens):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # dividing each matrics into small metrices for splitting into heads:\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # taking transpose to convert [Batch, Tokens, Heads, Dims] into [Batch, Heads, Tokens, Dims] shape:\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores = attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        # normalizing each and taking softmax to convert into attn weights:\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        # droping out:\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # transpose back to [batch, tokens, heads, dims]:\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # .contiguous() rearranges it in memory for efficiency before the next .view(). and .view() combine all the small metrices into al large matric back. 2, 6, 4 .\n",
        "        context_vec = self.out_proj(context_vec) # final check and if needed convert to original shape.\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "S0G5zPwTsMPC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],\n",
        "     [0.55, 0.87, 0.66],\n",
        "     [0.57, 0.85, 0.64],\n",
        "     [0.22, 0.58, 0.33],\n",
        "     [0.77, 0.25, 0.10],\n",
        "     [0.05, 0.80, 0.55]]\n",
        ")\n",
        "\n",
        "# creating a batch of 2 sequences\n",
        "batch = torch.stack((inputs, inputs), dim=0)"
      ],
      "metadata": {
        "id": "_OPeURU4YrAb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = 3\n",
        "d_out = 4\n",
        "context_length = 6\n",
        "dropout = 0.1\n",
        "num_heads = 4"
      ],
      "metadata": {
        "id": "bzTRe_BEGJLW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads)\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(\"Input shape:\", batch.shape)\n",
        "print(\"Output shape:\", context_vecs.shape)\n",
        "print(\"\\nOutput values:\")\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzWkaEd0Ghaw",
        "outputId": "e4fd8bad-abf6-417f-bcd8-9eba054bcde0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 6, 3])\n",
            "Output shape: torch.Size([2, 6, 4])\n",
            "\n",
            "Output values:\n",
            "tensor([[[ 0.1184,  0.3252, -0.0870, -0.5899],\n",
            "         [ 0.0076,  0.3391, -0.0776, -0.4197],\n",
            "         [-0.0287,  0.3427, -0.0744, -0.3635],\n",
            "         [-0.0438,  0.3239, -0.0731, -0.3321],\n",
            "         [-0.0318,  0.2762, -0.0816, -0.3648],\n",
            "         [-0.0538,  0.3060, -0.0716, -0.3083]],\n",
            "\n",
            "        [[ 0.1184,  0.3252, -0.0870, -0.5899],\n",
            "         [-0.1006,  0.3153, -0.0877, -0.3057],\n",
            "         [-0.0287,  0.3427, -0.0744, -0.3635],\n",
            "         [-0.0388,  0.2975, -0.0805, -0.3573],\n",
            "         [ 0.0263,  0.3174, -0.0555, -0.3554],\n",
            "         [-0.0468,  0.2723, -0.0740, -0.3175]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qxGR1UlHJIJ"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}